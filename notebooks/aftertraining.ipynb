{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытаемся улучшить CER с 16.624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:56:52.523672Z",
     "iopub.status.busy": "2025-05-06T17:56:52.522950Z",
     "iopub.status.idle": "2025-05-06T17:56:59.754960Z",
     "shell.execute_reply": "2025-05-06T17:56:59.754180Z",
     "shell.execute_reply.started": "2025-05-06T17:56:52.523645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install jiwer\n",
    "\n",
    "!pip install pyctcdecode\n",
    "# !pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, random, copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import Resample, MelSpectrogram, FrequencyMasking, TimeMasking\n",
    "from jiwer import cer\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "\n",
    "DATA_DIR   = '/kaggle/input/asr-numbers-recognition-in-russian'\n",
    "TRAIN_CSV  = os.path.join(DATA_DIR, 'train.csv')\n",
    "DEV_CSV    = os.path.join(DATA_DIR, 'dev.csv')\n",
    "CHECKPOINT = '/kaggle/input/16_624model/pytorch/default/1/model_best_2.pth'\n",
    "SAVE_PATH  = 'model_finetuned.pth'\n",
    "LM_PATH    = None\n",
    "\n",
    "\n",
    "df_tr = pd.read_csv(TRAIN_CSV); df_tr.transcription = df_tr.transcription.astype(str)\n",
    "df_dev = pd.read_csv(DEV_CSV); df_dev.transcription = df_dev.transcription.astype(str)\n",
    "\n",
    "def build_vocab(texts):\n",
    "    chars = sorted(set(''.join(texts)))\n",
    "    c2i = {c:i+1 for i,c in enumerate(chars)}   # 0 — blank\n",
    "    i2c = {i:c for c,i in c2i.items()}\n",
    "    return c2i, i2c\n",
    "\n",
    "def text_to_indices(t, c2i):\n",
    "    return [c2i.get(c,0) for c in t]\n",
    "\n",
    "char2idx, idx2char = build_vocab(df_tr.transcription)\n",
    "vocab_size = len(char2idx) + 1\n",
    "\n",
    "resampler    = Resample(orig_freq=24000, new_freq=16000)\n",
    "mel_spec     = MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "spec_augment = nn.Sequential(\n",
    "    FrequencyMasking(freq_mask_param=15),\n",
    "    TimeMasking(time_mask_param=35),\n",
    ")\n",
    "\n",
    "def speed_perturb(wav, sr):\n",
    "    rate = random.choice([0.9, 1.0, 1.1])\n",
    "    wav2 = F.resample(wav, orig_freq=sr, new_freq=int(sr*rate))\n",
    "    return wav2, int(sr*rate)\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, df, base, c2i, augment=False):\n",
    "        self.df, self.base, self.c2i = df, base, c2i\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        wav, sr = torchaudio.load(os.path.join(self.base, row.filename))\n",
    "        if self.augment:\n",
    "            wav, sr = speed_perturb(wav, sr)\n",
    "        if sr != 16000:\n",
    "            wav = resampler(wav)\n",
    "        spec = mel_spec(wav).squeeze(0).transpose(0,1)\n",
    "        if self.augment:\n",
    "            spec = spec_augment(spec)\n",
    "        tgt = torch.tensor(text_to_indices(row.transcription, self.c2i), dtype=torch.long)\n",
    "        return spec, tgt\n",
    "\n",
    "def collate_fn(batch):\n",
    "    specs, tgts = zip(*batch)\n",
    "    lengths = [s.size(0) for s in specs]\n",
    "    maxL = max(lengths)\n",
    "    B, M = len(specs), specs[0].size(1)\n",
    "    pad = torch.zeros(B, maxL, M)\n",
    "    for i, s in enumerate(specs):\n",
    "        pad[i, :lengths[i]] = s\n",
    "    tgt_lens = torch.tensor([t.size(0) for t in tgts])\n",
    "    tgts_cat = torch.cat(tgts)\n",
    "    return pad.transpose(1,2), torch.tensor(lengths), tgts_cat, tgt_lens\n",
    "\n",
    "tr_loader = DataLoader(\n",
    "    ASRDataset(df_tr, DATA_DIR, char2idx, augment=True),\n",
    "    batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "dv_loader = DataLoader(\n",
    "    ASRDataset(df_dev, DATA_DIR, char2idx, augment=False),\n",
    "    batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, V):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1,32,3,padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(64,64,3,padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(64*16,192, num_layers=2,\n",
    "                           batch_first=True,bidirectional=True)\n",
    "        self.classifier = nn.Linear(192*2, V)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # B×1×T×M\n",
    "        x = self.cnn(x)\n",
    "        B,C,H,W = x.size()\n",
    "        x = x.permute(0,3,1,2).reshape(B,W,C*H)\n",
    "        x,_ = self.rnn(x)\n",
    "        return self.classifier(x).log_softmax(2)\n",
    "\n",
    "labels = [\"\"] + [idx2char[i] for i in range(1, vocab_size)]\n",
    "decoder = build_ctcdecoder(labels, LM_PATH)\n",
    "\n",
    "def beam_decode(logp):\n",
    "    probs = logp.exp().cpu().numpy()\n",
    "    return decoder.decode_beams(probs, beam_width=50)[0][0]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ASRModel(vocab_size).to(device)\n",
    "model.load_state_dict(torch.load(CHECKPOINT, map_location=device))\n",
    "model.eval()\n",
    "refs, hyps = [], []\n",
    "with torch.no_grad():\n",
    "    for specs, lens, tgts, t_lens in dv_loader:\n",
    "        specs = specs.to(device)\n",
    "        out = model(specs)\n",
    "        off = 0\n",
    "        for i, L in enumerate(t_lens.tolist()):\n",
    "            hyp = beam_decode(out[i])\n",
    "            ref = \"\".join(idx2char[x] for x in tgts[off:off+L].tolist())\n",
    "            hyps.append(hyp); refs.append(ref)\n",
    "            off += L\n",
    "print(\"Baseline CER =\", cer(refs, hyps))\n",
    "\n",
    "# freeze CNN если нужно\n",
    "for p in model.cnn.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "EPOCH = 20\n",
    "\n",
    "opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                 lr=1e-5)\n",
    "sched = OneCycleLR(opt, max_lr=5e-5,\n",
    "                   total_steps=EPOCH * len(tr_loader))\n",
    "ctc = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "best_cer, best_state = float('inf'), None\n",
    "for ep in range(1, EPOCH+1):\n",
    "    model.train()\n",
    "    for specs, lens, tgts, t_lens in tqdm(tr_loader, desc=f\"Train {ep}\"):\n",
    "        specs, tgts = specs.to(device), tgts.to(device)\n",
    "        opt.zero_grad()\n",
    "        logit = model(specs)\n",
    "        loss = ctc(logit.permute(1,0,2), tgts, lens, t_lens)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        opt.step(); sched.step()\n",
    "\n",
    "    model.eval()\n",
    "    refs, hyps = [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, lens, tgts, t_lens in dv_loader:\n",
    "            specs = specs.to(device)\n",
    "            out = model(specs)\n",
    "            off = 0\n",
    "            for i, L in enumerate(t_lens.tolist()):\n",
    "                hyp = beam_decode(out[i])\n",
    "                ref = \"\".join(idx2char[x] for x in tgts[off:off+L].tolist())\n",
    "                hyps.append(hyp); refs.append(ref)\n",
    "                off += L\n",
    "    sc = cer(refs, hyps)\n",
    "    print(f\"Epoch {ep} → CER={sc:.4f}\")\n",
    "    if sc < best_cer:\n",
    "        best_cer, best_state = sc, copy.deepcopy(model.state_dict())\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Done. Best CER={best_cer:.4f}. Saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import os, random, copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import Resample, MelSpectrogram, FrequencyMasking, TimeMasking\n",
    "from jiwer import cer\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "DATA_DIR   = '/kaggle/input/asr-numbers-recognition-in-russian'\n",
    "TRAIN_CSV  = os.path.join(DATA_DIR, 'train.csv')\n",
    "DEV_CSV    = os.path.join(DATA_DIR, 'dev.csv')\n",
    "BEST_CKPT  = 'model_finetuned.pth' #'/kaggle/input/16_624model/pytorch/default/1/model_best_2.pth'  \n",
    "SAVE_PATH  = 'model_finetuned_30ep_.pth'\n",
    "LM_PATH    = None\n",
    "\n",
    "df_tr = pd.read_csv(TRAIN_CSV); df_tr.transcription = df_tr.transcription.astype(str)\n",
    "df_dev= pd.read_csv(DEV_CSV);  df_dev.transcription= df_dev.transcription.astype(str)\n",
    "\n",
    "def build_vocab(texts):\n",
    "    chars = sorted(set(''.join(texts)))\n",
    "    c2i = {c:i+1 for i,c in enumerate(chars)}  # 0 — blank\n",
    "    i2c = {i:c for c,i in c2i.items()}\n",
    "    return c2i, i2c\n",
    "\n",
    "def text_to_indices(t, c2i):\n",
    "    return [c2i.get(c,0) for c in t]\n",
    "\n",
    "char2idx, idx2char = build_vocab(df_tr.transcription)\n",
    "vocab_size = len(char2idx) + 1\n",
    "\n",
    "resampler    = Resample(orig_freq=24000, new_freq=16000)\n",
    "mel_spec     = MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "spec_augment = nn.Sequential(\n",
    "    FrequencyMasking(freq_mask_param=10),\n",
    "    TimeMasking(time_mask_param=20),\n",
    ")\n",
    "def speed_perturb(wav, sr):\n",
    "    rate = random.choice([0.9, 1.0, 1.1])\n",
    "    wav2 = F.resample(wav, orig_freq=sr, new_freq=int(sr*rate))\n",
    "    return wav2, int(sr*rate)\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, df, base, c2i, augment=False):\n",
    "        self.df, self.base, self.c2i = df, base, c2i\n",
    "        self.augment = augment\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        wav, sr = torchaudio.load(os.path.join(self.base, row.filename))\n",
    "        if self.augment:\n",
    "            wav, sr = speed_perturb(wav, sr)\n",
    "        if sr != 16000:\n",
    "            wav = resampler(wav)\n",
    "        spec = mel_spec(wav).squeeze(0).transpose(0,1)\n",
    "        if self.augment:\n",
    "            spec = spec_augment(spec)\n",
    "        tgt = torch.tensor(text_to_indices(row.transcription, self.c2i), dtype=torch.long)\n",
    "        return spec, tgt\n",
    "\n",
    "def collate_fn(batch):\n",
    "    specs, tgts = zip(*batch)\n",
    "    lengths = [s.size(0) for s in specs]\n",
    "    maxL = max(lengths); B,M = len(specs), specs[0].size(1)\n",
    "    pad = torch.zeros(B, maxL, M)\n",
    "    for i,s in enumerate(specs):\n",
    "        pad[i,:lengths[i]] = s\n",
    "    tgt_lens = torch.tensor([t.size(0) for t in tgts])\n",
    "    tgts_cat = torch.cat(tgts)\n",
    "    return pad.transpose(1,2), torch.tensor(lengths), tgts_cat, tgt_lens\n",
    "\n",
    "tr_loader = DataLoader(ASRDataset(df_tr, DATA_DIR, char2idx, augment=True),\n",
    "                       batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "dv_loader = DataLoader(ASRDataset(df_dev, DATA_DIR, char2idx, augment=False),\n",
    "                       batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, V):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1,32,3,padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(64,64,3,padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(64*16,192, num_layers=2,\n",
    "                           batch_first=True,bidirectional=True,\n",
    "                           dropout=0.3)\n",
    "        self.classifier = nn.Linear(192*2, V)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # B×1×T×M\n",
    "        x = self.cnn(x)\n",
    "        B,C,H,W = x.size()\n",
    "        x = x.permute(0,3,1,2).reshape(B,W,C*H)\n",
    "        x,_ = self.rnn(x)\n",
    "        return self.classifier(x).log_softmax(2)\n",
    "\n",
    "labels  = [\"\"] + [idx2char[i] for i in range(1, vocab_size)]\n",
    "decoder = build_ctcdecoder(labels, LM_PATH)\n",
    "def beam_decode(logp):\n",
    "    probs = logp.exp().cpu().numpy()\n",
    "    return decoder.decode_beams(probs, beam_width=50)[0][0]\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = ASRModel(vocab_size).to(DEVICE)\n",
    "model.load_state_dict(torch.load(BEST_CKPT, map_location=DEVICE))\n",
    "for p in model.parameters(): p.requires_grad = True\n",
    "\n",
    "head_params = list(model.classifier.parameters())\n",
    "\n",
    "backbone_params = [p for p in model.parameters() if all(p is not hp for hp in head_params)]\n",
    "\n",
    "opt  = optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': 1e-6, 'weight_decay':1e-5},\n",
    "    {'params': head_params,     'lr': 5e-5, 'weight_decay':1e-5},\n",
    "])\n",
    "EPOCH = 50\n",
    "steps = EPOCH * len(tr_loader)\n",
    "warm = int(0.1 * steps)\n",
    "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=warm, num_training_steps=steps)\n",
    "ctc   = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "best_cer, best_state = float('inf'), None\n",
    "for ep in range(1, EPOCH+1):\n",
    "    model.train()\n",
    "    for specs, lens, tgts, t_lens in tqdm(tr_loader, desc=f\"Train {ep:02d}/{EPOCH:02d}\"):\n",
    "        specs, tgts = specs.to(DEVICE), tgts.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        logits = model(specs)\n",
    "        loss   = ctc(logits.permute(1,0,2), tgts, lens, t_lens)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        opt.step(); sched.step()\n",
    "\n",
    "    model.eval()\n",
    "    refs, hyps = [], []\n",
    "    with torch.no_grad():\n",
    "        for specs, lens, tgts, t_lens in dv_loader:\n",
    "            specs = specs.to(DEVICE)\n",
    "            out   = model(specs)\n",
    "            off   = 0\n",
    "            for i, L in enumerate(t_lens.tolist()):\n",
    "                hy = beam_decode(out[i])\n",
    "                rf = \"\".join(idx2char[x] for x in tgts[off:off+L].tolist())\n",
    "                hyps.append(hy); refs.append(rf)\n",
    "                off += L\n",
    "    sc = cer(refs, hyps)\n",
    "    print(f\"Epoch {ep:02d} → CER={sc:.4f}\")\n",
    "    if sc < best_cer:\n",
    "        best_cer, best_state = sc, copy.deepcopy(model.state_dict())\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Done. Best CER={best_cer:.4f}. Saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Всего параметров:      {total_params:,}\")\n",
    "print(f\"Обучаемых параметров:  {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's make a csv for submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:22:32.503582Z",
     "iopub.status.busy": "2025-05-07T04:22:32.503310Z",
     "iopub.status.idle": "2025-05-07T04:22:38.421067Z",
     "shell.execute_reply": "2025-05-07T04:22:38.420339Z",
     "shell.execute_reply.started": "2025-05-07T04:22:32.503563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:22:47.385183Z",
     "iopub.status.busy": "2025-05-07T04:22:47.384896Z",
     "iopub.status.idle": "2025-05-07T04:22:50.296760Z",
     "shell.execute_reply": "2025-05-07T04:22:50.296015Z",
     "shell.execute_reply.started": "2025-05-07T04:22:47.385161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2.82M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from jiwer import cer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "\n",
    "DATA_DIR = '/kaggle/input/asr-numbers-recognition-in-russian'\n",
    "df_train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "df_train['transcription'] = df_train['transcription'].astype(str)\n",
    "df_dev   = pd.read_csv(os.path.join(DATA_DIR, 'dev.csv'))\n",
    "df_dev ['transcription'] = df_dev ['transcription'].astype(str)\n",
    "\n",
    "def text_to_indices(text, char2idx):\n",
    "    return [char2idx[c] for c in text]\n",
    "\n",
    "def build_vocab(transcripts):\n",
    "    texts = transcripts.astype(str)\n",
    "    chars = sorted(set(''.join(texts)))\n",
    "    char2idx = {c: i+1 for i, c in enumerate(chars)}  # 0 — CTC blank\n",
    "    idx2char = {i: c for c, i in char2idx.items()}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "char2idx, idx2char = build_vocab(df_train['transcription'])\n",
    "vocab_size = len(char2idx) + 1  # include CTC blank\n",
    "\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=24000, new_freq=16000)\n",
    "mel_spec  = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, df, data_base, char2idx):\n",
    "        self.df = df\n",
    "        self.base = data_base\n",
    "        self.char2idx = char2idx\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        wav, sr = torchaudio.load(os.path.join(self.base, row['filename']))\n",
    "        if sr != 16000:\n",
    "            wav = resampler(wav)\n",
    "        spec = mel_spec(wav).squeeze(0).transpose(0,1)  # T x n_mels\n",
    "        target = torch.tensor(text_to_indices(row['transcription'], self.char2idx), dtype=torch.long)\n",
    "        return spec, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    specs, targets = zip(*batch)\n",
    "    spec_lens = [s.size(0) for s in specs]\n",
    "    max_spec  = max(spec_lens)\n",
    "    padded_specs = torch.zeros(len(specs), max_spec, specs[0].size(1))\n",
    "    for i, s in enumerate(specs):\n",
    "        padded_specs[i, :s.size(0)] = s\n",
    "    tgt_lens = [t.size(0) for t in targets]\n",
    "    targets_cat = torch.cat(targets)\n",
    "    return padded_specs.transpose(1,2), torch.tensor(spec_lens), targets_cat, torch.tensor(tgt_lens)\n",
    "\n",
    "train_ds = ASRDataset(df_train, DATA_DIR, char2idx)\n",
    "dev_ds   = ASRDataset(df_dev,   DATA_DIR, char2idx)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(32,64,3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(64,64,3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=64*16,\n",
    "            hidden_size=192,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(192*2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (B,1,n_mels,T)\n",
    "        x = self.cnn(x)     # (B,64,16,T′)\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.permute(0,3,1,2).reshape(B, W, C*H)  # (B,T′,1024)\n",
    "        x, _ = self.rnn(x)                       # (B,T′,384)\n",
    "        x = self.classifier(x)                   # (B,T′,vocab)\n",
    "        return x.log_softmax(2)\n",
    "\n",
    "model = ASRModel(vocab_size)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {num_params/1e6:.2f}M\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "ctc_loss  = nn.CTCLoss(blank=0, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:26:15.960153Z",
     "iopub.status.busy": "2025-05-07T04:26:15.959874Z",
     "iopub.status.idle": "2025-05-07T04:26:16.063110Z",
     "shell.execute_reply": "2025-05-07T04:26:16.062273Z",
     "shell.execute_reply.started": "2025-05-07T04:26:15.960133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1945806015.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/kaggle/input/cer_0.1592/pytorch/default/1/model_finetuned_30ep_.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ASRModel(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rnn): LSTM(1024, 192, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (classifier): Linear(in_features=384, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/kaggle/input/cer_0.1592/pytorch/default/1/model_finetuned_30ep_.pth', map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:27:24.184890Z",
     "iopub.status.busy": "2025-05-07T04:27:24.184431Z",
     "iopub.status.idle": "2025-05-07T04:27:24.189492Z",
     "shell.execute_reply": "2025-05-07T04:27:24.188749Z",
     "shell.execute_reply.started": "2025-05-07T04:27:24.184868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def greedy_decode(log_probs, blank=0):\n",
    "    # log_probs: Tensor (T, vocab)\n",
    "    indices = log_probs.argmax(dim=1).cpu().tolist()\n",
    "    tokens, prev = [], None\n",
    "    for idx in indices:\n",
    "        if idx!=prev and idx!=blank:\n",
    "            tokens.append(idx)\n",
    "        prev = idx\n",
    "    return ''.join(idx2char[i] for i in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:27:27.885527Z",
     "iopub.status.busy": "2025-05-07T04:27:27.885238Z",
     "iopub.status.idle": "2025-05-07T04:29:09.870055Z",
     "shell.execute_reply": "2025-05-07T04:29:09.869341Z",
     "shell.execute_reply.started": "2025-05-07T04:27:27.885508Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 2582/2582 [01:41<00:00, 25.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in transcription: 0\n",
      "Saved → predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "BASE     = '/kaggle/input/asr-numbers-recognition-in-russian'\n",
    "TEST_CSV = os.path.join(BASE, 'test.csv')\n",
    "OUT_CSV  = 'predictions.csv'\n",
    "\n",
    "df = pd.read_csv(TEST_CSV)\n",
    "assert 'filename' in df.columns\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "for fn in tqdm(df['filename'], desc='Inference'):\n",
    "    wav, sr = torchaudio.load(os.path.join(BASE, fn))\n",
    "    if sr != 16000:\n",
    "        wav = resampler(wav)\n",
    "    spec = mel_spec(wav).squeeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logp = model(spec)\n",
    "    preds.append(greedy_decode(logp[0]))\n",
    "\n",
    "assert len(preds) == len(df), f\"inference count mismatch: {len(preds)} vs {len(df)}\"\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    'filename': df['filename'],\n",
    "    'transcription': preds\n",
    "})\n",
    "\n",
    "out['transcription'] = out['transcription'].fillna(0)\n",
    "\n",
    "n_null = out['transcription'].isna().sum()\n",
    "print(f\"Nulls in transcription: {n_null}\")\n",
    "\n",
    "out['transcription'] = pd.to_numeric(out['transcription'], errors='coerce').fillna(0).astype(int)\n",
    "out['transcription'] = out['transcription'].apply(lambda x: int(x) if x >= 0 else 0)\n",
    "\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved → {OUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11819802,
     "sourceId": 99036,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 331093,
     "modelInstanceId": 310723,
     "sourceId": 376082,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 334228,
     "modelInstanceId": 313814,
     "sourceId": 379693,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
